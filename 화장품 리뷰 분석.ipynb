{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 화장품 리뷰 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드를 진행함에 있어 konlpy를 사용하기 위해 java jdk를 설치 해야 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install konlpy\n",
    "\n",
    "pip install Cucco\n",
    "\n",
    "pip install wordcloud  - visual studio 설치 되어 있어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import graphviz             \n",
    "import matplotlib.font_manager as fm        \n",
    "import matplotlib as mpl\n",
    "import warnings  \n",
    "\n",
    "from konlpy.tag import Kkma       ; kkma = Kkma()\n",
    "from konlpy.tag import Hannanum   ; hannanum = Hannanum()\n",
    "from konlpy.tag import Okt        ; t = Okt()\n",
    "from konlpy.tag import *\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from konlpy.tag import Okt; t = Okt()\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# 그래프 문자 깨지는 것 대처\n",
    "plt.rc('font', family='NanumGothic')\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 워닝 없애주는 것\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r= pd.read_excel('cos38.xlsx') # cos38은 화장품 홈페이지에서 리뷰를 크롤링한 파일\n",
    "df_r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_r.shape)\n",
    "df_r.head(5)  # df_r에서 5개의 행만을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r= pd.read_excel('cos38.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_r.shape)\n",
    "print(df_r.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list = df_r.content.values.tolist() # df_r의 리뷰들을 리스트 하나에 합쳐서 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_text = ''\n",
    "\n",
    "for each_line in content_list:\n",
    "    content_text = content_text + each_line + '\\n'   # 리스트 하나로 합쳤던 문장들 사이에 공백을 개행문자로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_ko = t.morphs(content_text)\n",
    "tokens_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko = nltk.Text(tokens_ko)   \n",
    "print(len(ko.tokens))          # 토큰 전체 갯수\n",
    "print(len(set(ko.tokens)))     # 토큰 unique 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko.vocab().most_common(100)    # 가장 많이 나온 단어 100개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 : 인터넷 검색 시 검색 용어로 사용하지 않는 단어. 관사, 전치사, 조사, 접속사 등 검색 색인 단어로 의미가 없는 단어\n",
    "stop_words = ['.','가',\"!\",'\\r\\n\\r\\n','\\r\\n','\\n','\\n ','요','답변','...','을','수','에','질문','제','를','이','도',\n",
    "                      '좋','1','는','로','으로','2','것','은','다',',','니다','대','들',\n",
    "                      '2017','들','데','..','의','때','겠','고','게','네요','한','일','할',\n",
    "                      '10','?','하는','06','주','려고','인데','거','좀','는데','~','ㅎㅎ',\n",
    "                      '하나','이상','20','뭐','까','있는','잘','습니다','다면','했','주려',\n",
    "                      '지','있','못','후','중','줄','6','과','어떤','기본','!!',\n",
    "                      '단어','라고','중요한','합','가요','....','보이','네','무지']\n",
    "\n",
    "tokens_ko = [each_word for each_word in tokens_ko\n",
    "             if each_word not in stop_words]\n",
    "\n",
    "ko = nltk.Text(tokens_ko)\n",
    "ko.vocab().most_common(50)   # 자주 사용된 50가지 단어 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC에 설치되어 있는 폰트 리스트를 모두 불러오기\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "font_list = fm.findSystemFonts(fontpaths = None, fontext = 'ttf')\n",
    "\n",
    "font_list[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프에서 한글 글자 및 마이너스 부호 깨짐 방지\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import font_manager, rc\n",
    "import platform\n",
    "\n",
    "if platform.system() == 'Windows':\n",
    "# 윈도우인 경우\n",
    "    font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "    rc('font', family=font_name)\n",
    "else:    \n",
    "# Mac 인 경우\n",
    "    rc('font', family='AppleGothic')\n",
    "    \n",
    "matplotlib.rcParams['axes.unicode_minus'] = False   \n",
    "#그래프에서 마이너스 기호가 표시되도록 하는 설정입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "ko.plot(50) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ko.vocab().most_common(300)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 워드클라우드를 그려보자\n",
    "wordcloud = WordCloud(font_path='c:/Windows/Fonts/malgun.ttf',\n",
    "                      relative_scaling = 0.2,\n",
    "                      #stopwords=STOPWORDS,\n",
    "                      background_color='white',\n",
    "                      ).generate_from_frequencies(dict(data))\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평점 전처리 함수\n",
    "def star_preprocessing(value) : \n",
    "#     value = int(text)\n",
    "\n",
    "    if value <= 3 :        # 평점이 3점 이상 일때, 아닐때 \n",
    "        return '0'\n",
    "    else :\n",
    "        return '1'      # 평점이 3점 이상일 경우 1이 return 되서 긍정이라는 판단 하도록 함\n",
    "\n",
    "# 형태소 분석을 위한 함수 tokenizer를 통해서 글자 하나하나 자름\n",
    "def tokenizer(text) :\n",
    "    okt = Okt()         # 텍스트가 들어가면 okt에서 형태소를 분해함\n",
    "    return okt.morphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step1_data_preprocessing() :\n",
    "    # 수집한 데이터를 읽어온다.\n",
    "    df = pd.read_excel('cos38.xlsx')\n",
    "\n",
    "    # 전처리를 수행한다.\n",
    "    df['rating'] = df['rating'].apply(star_preprocessing) # apply안에는 함수가 들어간다. 여기서는 star_preprocessing 함수가 들어감\n",
    "        # 이 함수를 거치면 점수가 1.2.3점은 0점 (부정) , 4.5점은 1점 (긍정)으로 바뀐다\n",
    "        \n",
    "      # 학습데이터와 테스트 데이터로 나눈다.\n",
    "    text_list = df['content'].tolist()\n",
    "    star_list = df['rating'].tolist()\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # 80%는 학습, 20%는 test\n",
    "    text_train, text_test, star_train, star_test = train_test_split(text_list, star_list, test_size=0.2, random_state=0)\n",
    "\n",
    "    return text_train, text_test, star_train, star_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step2_learning(X_train, y_train, X_test, y_test):\n",
    "    # 주어진 데이터를 단어 사전으로 만들고 각 단어의 빈도수를 계산한 후 벡터화 하는 객체 생성\n",
    "    tfidf = TfidfVectorizer(lowercase=False, tokenizer=tokenizer)\n",
    "        # 쪼갠단어를 긍정의 확률이 얼마 인지 부정이 얼마인지를 나타내줌 \n",
    "        \n",
    "    # 문장별 나오는 단어수 세서 수치화, 벡터화해서 학습을 시킨다.\n",
    "    logistic = LogisticRegression(C=10.0, penalty='l2', random_state=0)\n",
    "\n",
    "    pipe = Pipeline([('vect', tfidf), ('clf', logistic)])\n",
    "                    # logistic regression = 긍정, 부정 판단\n",
    "    \n",
    "    # pipeline = tfidf 와 logistic을 함께 쓰겠다는 것을 표현 \n",
    "    \n",
    "    # 학습한다.\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # 학습 정확도 측정\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    # 학습된 모델을 저장한다.\n",
    "    with open('pipe.dat', 'wb') as fp :\n",
    "        pickle.dump(pipe, fp)\n",
    "                # pickle 형식으로 저장\n",
    "    print('저장완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step3_using_model() :\n",
    "    # 객체를 복원한다.\n",
    "    with open('pipe.dat', 'rb') as fp:\n",
    "        pipe = pickle.load(fp)\n",
    "\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    while True :\n",
    "        text = input('리뷰를 작성해주세요 :')\n",
    "        if text == '끝':\n",
    "            print('프로그램 종료')\n",
    "            break\n",
    "        else: \n",
    "            str = [text]\n",
    "            # 예측 정확도\n",
    "            r1 = np.max(pipe.predict_proba(str) * 100)\n",
    "\n",
    "            # 예측 결과\n",
    "            r2 = pipe.predict(str)[0]\n",
    "                                \n",
    "            if r2 == '1' :\n",
    "                print('긍정적인 리뷰')\n",
    "            else :\n",
    "                print('부정적인 리뷰')\n",
    "\n",
    "            print('정확도 : %.3f' % r1)\n",
    "\n",
    "        \n",
    "# 학습 함수\n",
    "def learning() :\n",
    "    text_train, text_test, star_train, star_test = step1_data_preprocessing()\n",
    "    step2_learning(text_train, star_train, text_test, star_test)\n",
    "\n",
    "# 사용 함수\n",
    "def using() :\n",
    "    step3_using_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
